#!/usr/bin/env python3
"""
Sistema de actualizaciÃ³n automÃ¡tica diaria para enriquecimiento continuo de datos
"""

import asyncio
import logging
import schedule
import time
from datetime import datetime, timedelta
from typing import Dict, List
import threading
import json
import os

# Importar nuestros extractores e integradores
from advanced_daticos_extractor import AdvancedDaticosExtractor
from daticos_data_integrator import DaticosDataIntegrator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DailyAutoUpdater:
    """
    Sistema que ejecuta actualizaciones automÃ¡ticas diarias para:
    1. Extraer nuevos datos de Daticos
    2. Enriquecer datos existentes
    3. Actualizar informaciÃ³n de registros existentes
    4. Generar reportes de actualizaciÃ³n
    """
    
    def __init__(self):
        self.extractor = AdvancedDaticosExtractor()
        self.integrator = DaticosDataIntegrator()
        self.update_running = False
        self.last_update_time = None
        self.update_stats = {}
        
        # ConfiguraciÃ³n de horarios
        self.config = {
            'daily_update_hour': '02:00',  # 2 AM cada dÃ­a
            'backup_update_hour': '14:00',  # 2 PM backup
            'max_daily_extractions': 1000,  # LÃ­mite de extracciones por dÃ­a
            'quality_threshold': 0.6,  # Umbral mÃ­nimo de calidad de datos
            'enable_automatic_updates': True
        }
        
        self.load_config()
    
    def load_config(self):
        """Cargar configuraciÃ³n desde archivo si existe"""
        try:
            config_file = '/app/backend/auto_updater_config.json'
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                    self.config.update(user_config)
                    logger.info("ğŸ“ ConfiguraciÃ³n personalizada cargada")
        except Exception as e:
            logger.error(f"Error cargando configuraciÃ³n: {e}")
    
    def save_config(self):
        """Guardar configuraciÃ³n actual"""
        try:
            config_file = '/app/backend/auto_updater_config.json'
            with open(config_file, 'w') as f:
                json.dump(self.config, f, indent=2)
            logger.info("ğŸ’¾ ConfiguraciÃ³n guardada")
        except Exception as e:
            logger.error(f"Error guardando configuraciÃ³n: {e}")
    
    async def run_daily_update(self):
        """Ejecutar actualizaciÃ³n diaria completa"""
        if self.update_running:
            logger.warning("âš ï¸  ActualizaciÃ³n ya en progreso, omitiendo...")
            return
        
        try:
            self.update_running = True
            self.last_update_time = datetime.utcnow()
            
            logger.info("ğŸš€ Iniciando actualizaciÃ³n diaria automÃ¡tica...")
            
            # Fase 1: ExtracciÃ³n de nuevos datos
            extraction_stats = await self.perform_daily_extraction()
            
            # Fase 2: IntegraciÃ³n de datos extraÃ­dos
            integration_stats = await self.perform_daily_integration()
            
            # Fase 3: Enriquecimiento de datos existentes
            enrichment_stats = await self.perform_data_enrichment()
            
            # Fase 4: Limpieza y optimizaciÃ³n
            cleanup_stats = await self.perform_daily_cleanup()
            
            # Compilar estadÃ­sticas finales
            self.update_stats = {
                'timestamp': datetime.utcnow().isoformat(),
                'extraction': extraction_stats,
                'integration': integration_stats,
                'enrichment': enrichment_stats,
                'cleanup': cleanup_stats,
                'total_duration_minutes': (datetime.utcnow() - self.last_update_time).total_seconds() / 60
            }
            
            # Guardar reporte de actualizaciÃ³n
            await self.save_update_report()
            
            logger.info(f"ğŸ‰ ActualizaciÃ³n diaria completada en {self.update_stats['total_duration_minutes']:.2f} minutos")
            
        except Exception as e:
            logger.error(f"âŒ Error durante actualizaciÃ³n diaria: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.update_running = False
    
    async def perform_daily_extraction(self) -> Dict:
        """Realizar extracciÃ³n diaria de datos"""
        logger.info("ğŸ” Fase 1: ExtracciÃ³n diaria de datos...")
        
        try:
            # Configurar extractor para bÃºsqueda incremental
            extraction_data = await self.extractor.extract_all_endpoint_data()
            
            if extraction_data:
                # Guardar datos con timestamp diario
                daily_file = f'/app/backend/daily_extractions/daticos_daily_{datetime.now().strftime("%Y%m%d")}.json'
                os.makedirs('/app/backend/daily_extractions', exist_ok=True)
                
                with open(daily_file, 'w', encoding='utf-8') as f:
                    json.dump(extraction_data, f, indent=2, ensure_ascii=False)
                
                stats = {
                    'success': True,
                    'total_records': extraction_data.get('total_records', 0),
                    'endpoints_processed': len(extraction_data.get('endpoints_explored', {})),
                    'file_saved': daily_file
                }
                
                logger.info(f"âœ… ExtracciÃ³n completada: {stats['total_records']} registros")
                return stats
            else:
                return {'success': False, 'error': 'No data extracted'}
                
        except Exception as e:
            logger.error(f"Error en extracciÃ³n diaria: {e}")
            return {'success': False, 'error': str(e)}
    
    async def perform_daily_integration(self) -> Dict:
        """Integrar datos extraÃ­dos diariamente"""
        logger.info("ğŸ”„ Fase 2: IntegraciÃ³n diaria de datos...")
        
        try:
            # Buscar archivo de extracciÃ³n del dÃ­a
            daily_file = f'/app/backend/daily_extractions/daticos_daily_{datetime.now().strftime("%Y%m%d")}.json'
            
            if os.path.exists(daily_file):
                # Usar integrador con el archivo especÃ­fico del dÃ­a
                integration_result = await self.integrator.process_and_integrate_data()
                
                if integration_result.get('success'):
                    stats = integration_result.get('stats', {})
                    stats['source_file'] = daily_file
                    
                    logger.info(f"âœ… IntegraciÃ³n completada: {stats.get('total_processed', 0)} registros procesados")
                    return stats
                else:
                    return {'success': False, 'error': integration_result.get('error', 'Unknown error')}
            else:
                return {'success': False, 'error': 'No daily extraction file found'}
                
        except Exception as e:
            logger.error(f"Error en integraciÃ³n diaria: {e}")
            return {'success': False, 'error': str(e)}
    
    async def perform_data_enrichment(self) -> Dict:
        """Enriquecer datos existentes con informaciÃ³n adicional"""
        logger.info("ğŸ’ Fase 3: Enriquecimiento de datos existentes...")
        
        try:
            await self.integrator.initialize_db()
            
            # Obtener registros que necesitan enriquecimiento
            personas_col = self.integrator.db[self.integrator.collections['personas']]
            
            # Buscar registros con calidad baja o sin actualizar recientemente
            cutoff_date = (datetime.utcnow() - timedelta(days=7)).isoformat()
            
            records_to_enrich = await personas_col.find({
                '$or': [
                    {'enrichment.data_quality_score': {'$lt': self.config['quality_threshold']}},
                    {'updated_at': {'$lt': cutoff_date}},
                    {'enrichment.last_enriched': {'$exists': False}}
                ]
            }).limit(100).to_list(100)
            
            enriched_count = 0
            errors = 0
            
            for record in records_to_enrich:
                try:
                    # Re-enriquecer registro
                    if record.get('cedula'):
                        # Intentar obtener mÃ¡s informaciÃ³n de este registro
                        additional_data = await self.extractor.extract_consultation_by_cedula(record['cedula'])
                        
                        if additional_data.get('found'):
                            # Actualizar registro con nueva informaciÃ³n
                            update_data = {
                                'enrichment.last_enriched': datetime.utcnow().isoformat(),
                                'enrichment.enrichment_attempts': record.get('enrichment', {}).get('enrichment_attempts', 0) + 1
                            }
                            
                            # Agregar nueva informaciÃ³n encontrada
                            for key, value in additional_data.get('data', {}).items():
                                if value and not record.get(key):
                                    update_data[key] = value
                            
                            await personas_col.update_one(
                                {'id': record['id']},
                                {'$set': update_data}
                            )
                            
                            enriched_count += 1
                    
                    # Rate limiting
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"Error enriqueciendo registro {record.get('id', 'N/A')}: {e}")
                    errors += 1
                    continue
            
            stats = {
                'success': True,
                'records_processed': len(records_to_enrich),
                'successfully_enriched': enriched_count,
                'errors': errors
            }
            
            logger.info(f"âœ… Enriquecimiento completado: {enriched_count} registros mejorados")
            return stats
            
        except Exception as e:
            logger.error(f"Error en enriquecimiento: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            await self.integrator.close_connection()
    
    async def perform_daily_cleanup(self) -> Dict:
        """Realizar limpieza y optimizaciÃ³n diaria"""
        logger.info("ğŸ§¹ Fase 4: Limpieza y optimizaciÃ³n...")
        
        try:
            await self.integrator.initialize_db()
            
            cleanup_stats = {
                'duplicates_removed': 0,
                'empty_records_removed': 0,
                'indexes_optimized': True
            }
            
            # Remover registros duplicados basados en cÃ©dula
            personas_col = self.integrator.db[self.integrator.collections['personas']]
            
            # Encontrar duplicados por cÃ©dula
            duplicates_pipeline = [
                {'$group': {
                    '_id': '$cedula',
                    'ids': {'$push': '$id'},
                    'count': {'$sum': 1}
                }},
                {'$match': {'count': {'$gt': 1}}}
            ]
            
            duplicate_groups = await personas_col.aggregate(duplicates_pipeline).to_list(1000)
            
            for group in duplicate_groups:
                # Mantener solo el primer registro de cada cÃ©dula duplicada
                ids_to_remove = group['ids'][1:]  # Remover todos excepto el primero
                
                for id_to_remove in ids_to_remove:
                    await personas_col.delete_one({'id': id_to_remove})
                    cleanup_stats['duplicates_removed'] += 1
            
            # Remover registros con informaciÃ³n insuficiente
            empty_records = await personas_col.delete_many({
                '$and': [
                    {'cedula': {'$in': ['', None]}},
                    {'nombre': {'$in': ['', None]}},
                    {'telefono': {'$in': ['', None]}}
                ]
            })
            
            cleanup_stats['empty_records_removed'] = empty_records.deleted_count
            
            # Re-crear Ã­ndices para optimizaciÃ³n
            await self.integrator.create_indexes()
            
            logger.info(f"âœ… Limpieza completada: {cleanup_stats['duplicates_removed']} duplicados removidos")
            return cleanup_stats
            
        except Exception as e:
            logger.error(f"Error en limpieza: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            await self.integrator.close_connection()
    
    async def save_update_report(self):
        """Guardar reporte detallado de actualizaciÃ³n"""
        try:
            report_dir = '/app/backend/update_reports'
            os.makedirs(report_dir, exist_ok=True)
            
            report_file = f"{report_dir}/daily_update_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.update_stats, f, indent=2, ensure_ascii=False)
            
            # Mantener solo los Ãºltimos 30 reportes
            await self.cleanup_old_reports(report_dir)
            
            logger.info(f"ğŸ“Š Reporte guardado: {report_file}")
            
        except Exception as e:
            logger.error(f"Error guardando reporte: {e}")
    
    async def cleanup_old_reports(self, report_dir: str):
        """Limpiar reportes antiguos"""
        try:
            import glob
            
            report_files = glob.glob(f"{report_dir}/daily_update_*.json")
            report_files.sort(reverse=True)  # MÃ¡s recientes primero
            
            # Mantener solo los Ãºltimos 30
            for old_file in report_files[30:]:
                os.remove(old_file)
                
        except Exception as e:
            logger.error(f"Error limpiando reportes antiguos: {e}")
    
    def setup_scheduler(self):
        """Configurar programador de tareas diarias"""
        try:
            if self.config['enable_automatic_updates']:
                # ActualizaciÃ³n principal diaria
                schedule.every().day.at(self.config['daily_update_hour']).do(
                    lambda: asyncio.run(self.run_daily_update())
                )
                
                # ActualizaciÃ³n de respaldo
                schedule.every().day.at(self.config['backup_update_hour']).do(
                    lambda: asyncio.run(self.run_daily_update())
                )
                
                logger.info(f"â° Actualizaciones programadas para: {self.config['daily_update_hour']} y {self.config['backup_update_hour']}")
            else:
                logger.info("â¸ï¸  Actualizaciones automÃ¡ticas deshabilitadas")
                
        except Exception as e:
            logger.error(f"Error configurando programador: {e}")
    
    def run_scheduler(self):
        """Ejecutar el programador en un hilo separado"""
        def scheduler_thread():
            while True:
                schedule.run_pending()
                time.sleep(60)  # Verificar cada minuto
        
        thread = threading.Thread(target=scheduler_thread, daemon=True)
        thread.start()
        logger.info("ğŸ”„ Programador de actualizaciones iniciado")
    
    def get_update_status(self) -> Dict:
        """Obtener estado actual del sistema de actualizaciones"""
        return {
            'update_running': self.update_running,
            'last_update_time': self.last_update_time.isoformat() if self.last_update_time else None,
            'config': self.config,
            'last_stats': self.update_stats,
            'next_scheduled_update': schedule.jobs[0].next_run.isoformat() if schedule.jobs else None
        }
    
    async def force_update_now(self):
        """Forzar actualizaciÃ³n inmediata (para testing/admin)"""
        logger.info("ğŸ”¥ Forzando actualizaciÃ³n inmediata...")
        await self.run_daily_update()
        return self.update_stats

# Instancia global del actualizador
daily_updater = DailyAutoUpdater()

# Funciones para integraciÃ³n con FastAPI
def start_auto_updater():
    """Iniciar el sistema de actualizaciones automÃ¡ticas"""
    daily_updater.setup_scheduler()
    daily_updater.run_scheduler()
    logger.info("ğŸš€ Sistema de actualizaciones automÃ¡ticas iniciado")

def get_updater_status():
    """Obtener estado del actualizador para API"""
    return daily_updater.get_update_status()

async def force_update():
    """Forzar actualizaciÃ³n inmediata desde API"""
    return await daily_updater.force_update_now()

def update_config(new_config: Dict):
    """Actualizar configuraciÃ³n del actualizador"""
    daily_updater.config.update(new_config)
    daily_updater.save_config()
    daily_updater.setup_scheduler()  # Re-configurar programador
    return daily_updater.config

if __name__ == "__main__":
    # Para ejecutar standalone
    async def test_update():
        await daily_updater.run_daily_update()
    
    logger.info("ğŸ§ª Ejecutando actualizaciÃ³n de prueba...")
    asyncio.run(test_update())