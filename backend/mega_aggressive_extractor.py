"""
MEGA AGGRESSIVE EXTRACTOR - SISTEMA M√ÅS POTENTE
Extractor ULTRA AGRESIVO que usa TODAS las t√©cnicas posibles para extraer 5+ MILLONES DE REGISTROS

T√©cnicas implementadas:
- Session pooling con m√∫ltiples conexiones simult√°neas
- User-agent rotation para evitar detecci√≥n
- Request rate optimization con backoff inteligente
- Deep crawling con recursi√≥n
- Pattern matching avanzado
- Proxy rotation (simulado)
- Cache inteligente para evitar duplicados
- Persistence layer optimizada
- Real-time statistics
- Error recovery autom√°tico
- Session persistence y recovery

Meta: EXTRAER TODA LA BASE DE DATOS DE DATICOS (5+ MILLONES)
"""

import asyncio
import httpx
import logging
import re
import uuid
import random
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple
from bs4 import BeautifulSoup
from motor.motor_asyncio import AsyncIOMotorClient
import os
from dotenv import load_dotenv
from faker import Faker
import time as time_module
import string
from concurrent.futures import ThreadPoolExecutor
import hashlib

# Configurar logging ultra detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/backend/mega_aggressive_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()
fake = Faker('es_ES')

class MegaAggressiveExtractor:
    """
    MEGA AGGRESSIVE EXTRACTOR - EL M√ÅS POTENTE JAM√ÅS CREADO
    """
    
    def __init__(self):
        self.mongo_url = os.environ.get('MONGO_URL')
        self.db_name = os.environ.get('DB_NAME', 'test_database')
        self.client = None
        self.db = None
        
        # Pool de sesiones m√∫ltiples para m√°ximo paralelismo
        self.session_pool = {}
        self.base_url = "https://www.daticos.com"
        
        # TODAS las credenciales + t√©cnicas de rotaci√≥n
        self.credentials_pool = [
            {'username': 'CABEZAS', 'password': 'Hola2022', 'name': 'cabezas', 'priority': 1},
            {'username': 'Saraya', 'password': '12345', 'name': 'saraya', 'priority': 2}
        ]
        
        # User agents ultra diversos para evitar detecci√≥n
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0'
        ]
        
        # Estado de login para cada credencial
        self.login_status = {}
        self.login_sessions = {}
        
        # Cache inteligente para evitar duplicados
        self.extraction_cache = set()
        self.processed_urls = set()
        
        # Estad√≠sticas mega detalladas
        self.mega_stats = {
            'total_extraido': 0,
            'registros_nuevos_unicos': 0,
            'personas_fisicas_mega': 0,
            'personas_juridicas_mega': 0,
            'telefonos_cr_mega': 0,
            'emails_validados_mega': 0,
            'cedulas_fisicas_unicas': set(),
            'cedulas_juridicas_unicas': set(),
            'telefonos_unicos': set(),
            'emails_unicos': set(),
            'requests_realizadas': 0,
            'requests_exitosas': 0,
            'requests_fallidas': 0,
            'cache_hits': 0,
            'datos_duplicados_evitados': 0,
            'sessions_activas': 0,
            'recovery_attempts': 0,
            'deep_crawl_levels': 0,
            'patterns_encontrados': 0,
            'tiempo_total_segundos': 0,
            'velocidad_registros_por_segundo': 0,
            'mejor_credencial': None,
            'mejor_endpoint': None,
            'extraction_by_method': {},
            'errors_by_type': {}
        }
        
        # Endpoints MEGA EXPANDIDOS con t√©cnicas de discovery
        self.mega_endpoints = self.discover_all_possible_endpoints()
        
        # T√©rminos de b√∫squeda MEGA COMPREHENSIVOS con IA
        self.mega_search_terms = self.generate_mega_search_terms()
        
        # Patrones Costa Rica ultra espec√≠ficos y expandidos
        self.mega_cr_patterns = self.build_mega_cr_patterns()
        
        # Rate limiting inteligente
        self.rate_limiter = {
            'requests_per_minute': 200,  # Ultra agresivo pero inteligente
            'backoff_factor': 1.2,
            'max_backoff': 10,
            'current_delay': 0.1,
            'success_streak': 0,
            'error_streak': 0
        }
        
    def discover_all_possible_endpoints(self) -> List[str]:
        """Descubrir TODOS los endpoints posibles usando t√©cnicas avanzadas"""
        base_endpoints = [
            '/busced.php', '/busnom.php', '/buspat.php', '/bussoc.php',
            '/busemp.php', '/bustel.php', '/busdir.php', '/buslaboral.php',
            '/busmatri.php', '/buscredit.php', '/busactiv.php', '/busrep.php',
            '/buslic.php', '/busprop.php', '/busveh.php', '/busasoc.php',
            '/busalt.php', '/busext.php'
        ]
        
        # Generar variaciones y endpoints probables
        variations = []
        for endpoint in base_endpoints:
            base_name = endpoint.replace('.php', '')
            
            # Variaciones comunes
            variations.extend([
                f"{base_name}.php",
                f"{base_name}2.php",
                f"{base_name}_new.php",
                f"{base_name}_v2.php",
                f"{base_name}_search.php",
                f"{base_name}_query.php",
                f"{base_name}_list.php",
                f"{base_name}_all.php",
                f"{base_name}_advanced.php"
            ])
            
        # Endpoints adicionales probables
        additional_endpoints = [
            '/search.php', '/query.php', '/find.php', '/lookup.php',
            '/consulta.php', '/busqueda.php', '/datos.php', '/info.php',
            '/persona.php', '/empresa.php', '/juridica.php', '/fisica.php',
            '/telefono.php', '/email.php', '/direccion.php', '/ubicacion.php',
            '/vehiculo.php', '/propiedad.php', '/bien.php', '/asset.php',
            '/matrimonio.php', '/laboral.php', '/trabajo.php', '/empleo.php',
            '/credito.php', '/financiero.php', '/banco.php', '/prestamo.php',
            '/comercial.php', '/mercantil.php', '/negocio.php', '/actividad.php',
            '/registro.php', '/cedula.php', '/identificacion.php', '/documento.php'
        ]
        
        all_endpoints = list(set(base_endpoints + variations + additional_endpoints))
        logger.info(f"üîç Endpoints descubiertos: {len(all_endpoints)}")
        
        return all_endpoints
    
    def generate_mega_search_terms(self) -> List[str]:
        """Generar t√©rminos de b√∫squeda MEGA comprehensivos usando t√©cnicas avanzadas"""
        terms = []
        
        # Alfabeto completo
        terms.extend(list(string.ascii_uppercase))
        terms.extend(list(string.ascii_lowercase))
        
        # N√∫meros y combinaciones
        terms.extend([str(i) for i in range(100)])  # 0-99
        
        # Combinaciones de 2 y 3 caracteres m√°s probables
        common_2_char = ['SA', 'LA', 'EL', 'DE', 'CO', 'MA', 'AL', 'CA', 'SO', 'EM', 'IN', 'RE', 'TE', 'ES', 'EN', 'AN', 'OR', 'AR', 'ER', 'TO']
        terms.extend(common_2_char)
        
        common_3_char = ['SAN', 'LOS', 'LAS', 'DEL', 'COM', 'ING', 'ADM', 'GER', 'DIR', 'SUP', 'ASI', 'SEC', 'TEC', 'OPE', 'VEN', 'PRO']
        terms.extend(common_3_char)
        
        # A√±os completos
        terms.extend([str(year) for year in range(1950, 2025)])
        
        # Nombres m√°s comunes en Costa Rica
        nombres_cr = [
            'JOSE', 'MARIA', 'LUIS', 'ANA', 'CARLOS', 'JORGE', 'FRANCISCO', 'ROSA',
            'MANUEL', 'CARMEN', 'ANTONIO', 'LUCIA', 'RAFAEL', 'ELENA', 'MIGUEL',
            'PATRICIA', 'ROBERTO', 'SANDRA', 'FERNANDO', 'GLORIA', 'RICARDO',
            'ADRIANA', 'MARIO', 'MONICA', 'ALBERTO', 'SILVIA', 'OSCAR', 'TERESA'
        ]
        terms.extend(nombres_cr)
        
        # Apellidos m√°s comunes en Costa Rica
        apellidos_cr = [
            'RODRIGUEZ', 'GONZALEZ', 'HERNANDEZ', 'JIMENEZ', 'MARTINEZ', 'LOPEZ',
            'GARCIA', 'VARGAS', 'CASTRO', 'ROJAS', 'MORALES', 'GUTIERREZ',
            'VEGA', 'MENDEZ', 'FERNANDEZ', 'ARIAS', 'SOTO', 'CORDERO', 'ALVARADO',
            'BARRANTES', 'VILLALOBOS', 'MORA', 'CHACON', 'QUESADA', 'MADRIGAL'
        ]
        terms.extend(apellidos_cr)
        
        # T√©rminos comerciales y empresariales
        comerciales = [
            'COMERCIAL', 'EMPRESA', 'SOCIEDAD', 'SA', 'LIMITADA', 'LTDA', 'SRL',
            'CORPORACION', 'GROUP', 'INTERNACIONAL', 'NACIONAL', 'COSTA', 'RICA',
            'CENTRAL', 'PACIFICO', 'ATLANTICO', 'SERVICIOS', 'PRODUCTOS', 'INDUSTRIA',
            'CONSTRUCCION', 'INMOBILIARIA', 'FINANCIERA', 'CONSULTORIA', 'TECNOLOGIA'
        ]
        terms.extend(comerciales)
        
        # Sectores econ√≥micos
        sectores = [
            'AGRICULTURA', 'GANADERIA', 'PESCA', 'MINERIA', 'MANUFACTURA',
            'CONSTRUCCION', 'COMERCIO', 'TRANSPORTE', 'COMUNICACION', 'FINANCIERO',
            'SEGUROS', 'INMOBILIARIO', 'PROFESIONAL', 'ADMINISTRATIVO', 'EDUCACION',
            'SALUD', 'SOCIAL', 'ENTRETENIMIENTO', 'RESTAURANTE', 'HOTEL', 'TURISMO'
        ]
        terms.extend(sectores)
        
        # Ubicaciones geogr√°ficas espec√≠ficas de Costa Rica
        ubicaciones = [
            'SAN JOSE', 'CARTAGO', 'ALAJUELA', 'HEREDIA', 'GUANACASTE', 'PUNTARENAS',
            'LIMON', 'DESAMPARADOS', 'ESCAZU', 'SANTA ANA', 'CURRIDABAT', 'MORAVIA',
            'GOICOECHEA', 'TIBAS', 'MONTES OCA', 'LA UNION', 'BELEN', 'FLORES',
            'SANTO DOMINGO', 'TRES RIOS', 'GUADALUPE', 'CORONADO', 'PAVAS', 'HATILLO'
        ]
        terms.extend(ubicaciones)
        
        # Prefijos de c√©dulas costarricenses
        cedula_prefixes = []
        for i in range(1, 10):  # 1-0000-0000 to 9-9999-9999
            for j in range(0, 10):
                for k in range(0, 10):
                    cedula_prefixes.append(f"{i}-{j}{k}")
        terms.extend(cedula_prefixes[:200])  # Limitar para evitar sobrecarga
        
        # C√≥digos telef√≥nicos
        phone_prefixes = []
        for prefix in ['2', '4', '6', '7', '8']:
            for second in range(0, 10):
                phone_prefixes.append(f"{prefix}{second}")
        terms.extend(phone_prefixes)
        
        # Patrones de b√∫squeda especiales
        special_patterns = [
            '*', '?', '%', '_', '&', '+', '-', '=', '@', '#',
            '00', '01', '02', '03', '04', '05', '06', '07', '08', '09'
        ]
        terms.extend(special_patterns)
        
        logger.info(f"üîç T√©rminos mega generados: {len(set(terms))}")
        return list(set(terms))  # Eliminar duplicados
    
    def build_mega_cr_patterns(self) -> Dict:
        """Construir patrones mega espec√≠ficos para Costa Rica"""
        return {
            'cedulas_fisicas': [
                re.compile(r'[1-9]-\d{4}-\d{4}'),
                re.compile(r'[1-9]\d{8}'),  # Sin guiones
                re.compile(r'(?:cedula|c√©dula|id)[\s:]*([1-9]-\d{4}-\d{4})', re.IGNORECASE)
            ],
            'cedulas_juridicas': [
                re.compile(r'3-\d{3}-\d{6}'),
                re.compile(r'3\d{9}'),  # Sin guiones
                re.compile(r'(?:juridica|jur√≠dica|empresa)[\s:]*([3-]\d{3}-\d{6})', re.IGNORECASE)
            ],
            'telefonos_cr': [
                re.compile(r'\+506[\s-]?([2-8]\d{3}[\s-]?\d{4})'),
                re.compile(r'506[\s-]?([2-8]\d{3}[\s-]?\d{4})'),
                re.compile(r'([2]\d{3}[\s-]?\d{4})'),  # Fijos
                re.compile(r'([4]\d{3}[\s-]?\d{4})'),  # Nuevos fijos
                re.compile(r'([6-7]\d{3}[\s-]?\d{4})'),  # M√≥viles
                re.compile(r'([8]\d{3}[\s-]?\d{4})'),  # M√≥viles nuevos
                re.compile(r'(?:tel|tel√©fono|telefono|phone|cel|celular)[\s:]*([2-8]\d{3}[\s-]?\d{4})', re.IGNORECASE)
            ],
            'emails': [
                re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
                re.compile(r'(?:email|correo|mail)[\s:]*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})', re.IGNORECASE)
            ],
            'salarios': [
                re.compile(r'‚Ç°[\s]?(\d{1,3}(?:[,\.]\d{3})*(?:[,\.]\d{2})?)'),
                re.compile(r'(?:salario|sueldo|ingreso|pago)[\s:]*‚Ç°?[\s]?(\d{1,3}(?:[,\.]\d{3})*)', re.IGNORECASE),
                re.compile(r'(\d{1,3}(?:[,\.]\d{3})*)\s*(?:colones|CRC)', re.IGNORECASE)
            ],
            'direcciones': [
                re.compile(r'(?:direccion|direcci√≥n|domicilio)[\s:]*([^\n\r]{10,100})', re.IGNORECASE),
                re.compile(r'((?:San Jos√©|Cartago|Alajuela|Heredia|Guanacaste|Puntarenas|Lim√≥n)[^\n\r]{5,50})', re.IGNORECASE)
            ],
            'empresas': [
                re.compile(r'([A-Z\s]+(?:S\.?A\.?|LTDA|LIMITADA|SRL|CORP|CORPORATION|GROUP|INTERNACIONAL|NACIONAL))', re.IGNORECASE),
                re.compile(r'(?:empresa|compa√±ia|corporacion)[\s:]*([A-Z\s]{5,50})', re.IGNORECASE)
            ]
        }
    
    async def initialize_mega_system(self):
        """Inicializar sistema mega agresivo"""
        try:
            logger.info("üöÄ INICIALIZANDO MEGA AGGRESSIVE SYSTEM")
            
            # Conexi√≥n MongoDB optimizada
            self.client = AsyncIOMotorClient(
                self.mongo_url,
                maxPoolSize=50,  # Pool m√°s grande
                minPoolSize=10,
                maxIdleTimeMS=30000,
                serverSelectionTimeoutMS=5000
            )
            self.db = self.client[self.db_name]
            await self.db.command('ping')
            logger.info("‚úÖ MongoDB Mega Connection - OK")
            
            # Crear √≠ndices mega optimizados
            await self.create_mega_indexes()
            
            # Inicializar pool de sesiones m√∫ltiples
            await self.initialize_mega_session_pool()
            
            # Realizar login con todas las credenciales
            await self.mega_login_all_credentials()
            
            logger.info("üî• MEGA AGGRESSIVE SYSTEM INITIALIZED")
            logger.info(f"üéØ OBJETIVO: 5+ MILLONES DE REGISTROS")
            logger.info(f"üîê CREDENCIALES: {len(self.credentials_pool)} disponibles")
            logger.info(f"üåê ENDPOINTS: {len(self.mega_endpoints)} mega endpoints")
            logger.info(f"üîç T√âRMINOS: {len(self.mega_search_terms)} t√©rminos mega")
            logger.info(f"‚ö° SESIONES: Pool de {len(self.session_pool)} sesiones")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando mega system: {e}")
            return False
    
    async def create_mega_indexes(self):
        """Crear √≠ndices mega optimizados para 5M+ registros"""
        try:
            mega_indexes = {
                'personas_fisicas': [
                    ('cedula', 1), ('telefono', 1), ('email', 1), 
                    ('nombre', 'text'), ('primer_apellido', 1), ('segundo_apellido', 1),
                    ('provincia_id', 1), ('canton_id', 1), ('distrito_id', 1),
                    ('created_at', -1), ('fuente_mega', 1)
                ],
                'personas_juridicas': [
                    ('cedula_juridica', 1), ('telefono', 1), ('email', 1),
                    ('nombre_comercial', 'text'), ('razon_social', 'text'),
                    ('sector_negocio', 1), ('provincia_id', 1), ('created_at', -1),
                    ('fuente_mega', 1)
                ],
                'vehiculos_cr': [
                    ('cedula_propietario', 1), ('placa', 1), ('marca', 1),
                    ('modelo', 1), ('a√±o', 1), ('created_at', -1)
                ],
                'propiedades_cr': [
                    ('cedula_propietario', 1), ('folio_real', 1), ('provincia', 1),
                    ('canton', 1), ('valor_fiscal', 1), ('created_at', -1)
                ],
                'mega_extraction_data': [
                    ('extraction_hash', 1), ('created_at', -1), 
                    ('credencial_usada', 1), ('endpoint_origen', 1)
                ]
            }
            
            for collection, indexes in mega_indexes.items():
                for index_spec in indexes:
                    try:
                        if isinstance(index_spec, tuple) and len(index_spec) == 2:
                            field, order = index_spec
                            if order == 'text':
                                await self.db[collection].create_index([(field, 'text')], background=True)
                            else:
                                await self.db[collection].create_index([(field, order)], background=True)
                    except Exception as e:
                        # √çndice ya existe o error menor
                        pass
            
            logger.info("‚úÖ √çndices mega optimizados creados")
        except Exception as e:
            logger.error(f"‚ùå Error creando mega √≠ndices: {e}")
    
    async def initialize_mega_session_pool(self):
        """Inicializar pool de sesiones mega para paralelismo m√°ximo"""
        try:
            pool_size = 10  # 10 sesiones simult√°neas
            
            for i in range(pool_size):
                for cred in self.credentials_pool:
                    session_key = f"{cred['name']}_session_{i}"
                    
                    timeout = httpx.Timeout(60.0, connect=30.0)
                    session = httpx.AsyncClient(
                        timeout=timeout,
                        follow_redirects=True,
                        headers={
                            'User-Agent': random.choice(self.user_agents),
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                            'Accept-Language': 'es-CR,es;q=0.9,en;q=0.8',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'Connection': 'keep-alive',
                            'Cache-Control': 'max-age=0',
                            'Sec-Fetch-Dest': 'document',
                            'Sec-Fetch-Mode': 'navigate',
                            'Sec-Fetch-Site': 'none',
                            'Upgrade-Insecure-Requests': '1'
                        }
                    )
                    
                    self.session_pool[session_key] = {
                        'session': session,
                        'credential': cred,
                        'logged_in': False,
                        'last_used': None,
                        'request_count': 0,
                        'success_count': 0,
                        'error_count': 0
                    }
            
            self.mega_stats['sessions_activas'] = len(self.session_pool)
            logger.info(f"‚úÖ Pool de {len(self.session_pool)} sesiones mega inicializado")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando session pool: {e}")
    
    async def mega_login_all_credentials(self):
        """Login mega con todas las credenciales en todas las sesiones"""
        login_tasks = []
        
        for session_key, session_data in self.session_pool.items():
            task = self.mega_login_session(session_key, session_data)
            login_tasks.append(task)
        
        # Ejecutar todos los logins en paralelo
        results = await asyncio.gather(*login_tasks, return_exceptions=True)
        
        successful_logins = sum(1 for result in results if result is True)
        total_sessions = len(self.session_pool)
        
        logger.info(f"üîê Logins mega completados: {successful_logins}/{total_sessions}")
        
        if successful_logins == 0:
            logger.error("‚ùå CR√çTICO: Ning√∫n login fue exitoso")
            return False
        
        return True
    
    async def mega_login_session(self, session_key: str, session_data: Dict) -> bool:
        """Login individual para una sesi√≥n espec√≠fica"""
        try:
            session = session_data['session']
            cred = session_data['credential']
            
            # Obtener p√°gina de login
            login_page = await session.get(f"{self.base_url}/login.php")
            if login_page.status_code != 200:
                return False
            
            soup = BeautifulSoup(login_page.text, 'html.parser')
            
            # Preparar datos de login
            form_data = {
                'login': cred['username'],
                'password': cred['password'],
                'submit': 'Ingresar'
            }
            
            # Buscar campos hidden
            form = soup.find('form')
            if form:
                hidden_inputs = form.find_all('input', type='hidden')
                for inp in hidden_inputs:
                    if inp.get('name') and inp.get('value'):
                        form_data[inp.get('name')] = inp.get('value')
            
            # Realizar login
            login_response = await session.post(
                f"{self.base_url}/login.php",
                data=form_data,
                headers={'Content-Type': 'application/x-www-form-urlencoded'}
            )
            
            # Verificar √©xito
            if login_response.status_code == 200:
                response_text = login_response.text.lower()
                success_indicators = ['consultas individuales', 'consultas masivas', 'salir', 'logout.php']
                
                if any(indicator in response_text for indicator in success_indicators):
                    session_data['logged_in'] = True
                    session_data['last_used'] = datetime.utcnow()
                    logger.info(f"‚úÖ Login exitoso: {session_key} - {cred['username']}")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Error login {session_key}: {e}")
            return False
    
    async def extract_mega_aggressive_all_data(self, target_records=5000000):
        """EXTRACCI√ìN MEGA AGRESIVA DE TODA LA BASE DE DATOS (5M+)"""
        start_time = datetime.utcnow()
        logger.info(f"üî•üî•üî• INICIANDO MEGA AGGRESSIVE EXTRACTION üî•üî•üî•")
        logger.info(f"üéØ META: {target_records:,} registros")
        logger.info(f"‚ö° SESIONES ACTIVAS: {len(self.session_pool)}")
        logger.info(f"üåê ENDPOINTS A EXPLORAR: {len(self.mega_endpoints)}")
        logger.info(f"üîç T√âRMINOS DE B√öSQUEDA: {len(self.mega_search_terms)}")
        
        extracted_total = 0
        batch_size = 2000  # Lotes m√°s grandes
        concurrent_tasks = 20  # M√°ximo paralelismo
        
        # Crear tareas de extracci√≥n masiva en paralelo
        extraction_tasks = []
        
        # Dividir el trabajo en chunks para paralelizaci√≥n
        total_combinations = len(self.mega_endpoints) * len(self.mega_search_terms)
        chunk_size = max(1, total_combinations // concurrent_tasks)
        
        for i in range(0, len(self.mega_endpoints), max(1, len(self.mega_endpoints) // concurrent_tasks)):
            endpoint_chunk = self.mega_endpoints[i:i + max(1, len(self.mega_endpoints) // concurrent_tasks)]
            
            task = self.process_endpoint_chunk_mega(
                endpoint_chunk,
                target_records // concurrent_tasks,
                batch_size
            )
            extraction_tasks.append(task)
        
        # Ejecutar todas las tareas en paralelo con l√≠mite
        semaphore = asyncio.Semaphore(concurrent_tasks)
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        bounded_tasks = [bounded_task(task) for task in extraction_tasks]
        
        logger.info(f"üöÄ Ejecutando {len(bounded_tasks)} tareas paralelas...")
        
        # Procesar con timeout y manejo de errores
        results = await asyncio.gather(*bounded_tasks, return_exceptions=True)
        
        # Procesar resultados
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå Tarea {i} fall√≥: {result}")
                self.mega_stats['errors_by_type']['task_failure'] = self.mega_stats['errors_by_type'].get('task_failure', 0) + 1
            elif isinstance(result, int):
                extracted_total += result
                logger.info(f"‚úÖ Tarea {i} completada: +{result:,} registros")
        
        # Generar estad√≠sticas finales
        end_time = datetime.utcnow()
        self.mega_stats['tiempo_total_segundos'] = (end_time - start_time).total_seconds()
        self.mega_stats['total_extraido'] = extracted_total
        self.mega_stats['velocidad_registros_por_segundo'] = extracted_total / max(1, self.mega_stats['tiempo_total_segundos'])
        
        await self.generate_mega_final_report()
        
        logger.info(f"üéâüéâüéâ MEGA AGGRESSIVE EXTRACTION COMPLETADA! üéâüéâüéâ")
        logger.info(f"üìä TOTAL EXTRA√çDO: {extracted_total:,} registros")
        logger.info(f"‚è±Ô∏è TIEMPO: {self.mega_stats['tiempo_total_segundos']/60:.2f} minutos")
        logger.info(f"‚ö° VELOCIDAD: {self.mega_stats['velocidad_registros_por_segundo']:.1f} registros/segundo")
        
        return extracted_total
    
    async def process_endpoint_chunk_mega(self, endpoints: List[str], target_chunk: int, batch_size: int) -> int:
        """Procesar chunk de endpoints con m√°xima agresividad"""
        extracted_count = 0
        
        try:
            for endpoint in endpoints:
                if extracted_count >= target_chunk:
                    break
                
                logger.info(f"üî• Procesando endpoint mega: {endpoint}")
                
                # Usar todas las sesiones disponibles para este endpoint
                endpoint_tasks = []
                
                for session_key, session_data in self.session_pool.items():
                    if not session_data['logged_in']:
                        continue
                    
                    if extracted_count >= target_chunk:
                        break
                    
                    # Crear tarea para esta sesi√≥n espec√≠fica
                    task = self.extract_from_session_mega(
                        session_key, 
                        endpoint, 
                        target_chunk // len(self.session_pool)
                    )
                    endpoint_tasks.append(task)
                
                # Ejecutar todas las sesiones en paralelo para este endpoint
                if endpoint_tasks:
                    session_results = await asyncio.gather(*endpoint_tasks, return_exceptions=True)
                    
                    for session_result in session_results:
                        if isinstance(session_result, int):
                            extracted_count += session_result
                        elif isinstance(session_result, Exception):
                            logger.error(f"‚ùå Error en sesi√≥n: {session_result}")
                
                # Rate limiting inteligente
                await self.intelligent_rate_limit()
                
                if extracted_count % 10000 == 0:
                    logger.info(f"üìà Progreso chunk: {extracted_count:,}/{target_chunk:,}")
        
        except Exception as e:
            logger.error(f"‚ùå Error en chunk de endpoints: {e}")
        
        return extracted_count
    
    async def extract_from_session_mega(self, session_key: str, endpoint: str, target_session: int) -> int:
        """Extraer datos usando una sesi√≥n espec√≠fica con m√°xima agresividad"""
        session_data = self.session_pool[session_key]
        session = session_data['session']
        extracted_count = 0
        
        try:
            # Usar subconjunto de t√©rminos para esta sesi√≥n
            term_chunk_size = max(1, len(self.mega_search_terms) // 10)
            session_terms = self.mega_search_terms[:term_chunk_size]
            
            for term in session_terms:
                if extracted_count >= target_session:
                    break
                
                try:
                    # M√∫ltiples m√©todos de extracci√≥n
                    extraction_methods = [
                        {'q': term}, {'search': term}, {'term': term},
                        {'query': term}, {'buscar': term}, {'texto': term},
                        {'criterio': term}, {'nombre': term}, {'cedula': term}
                    ]
                    
                    for method in extraction_methods:
                        if extracted_count >= target_session:
                            break
                        
                        # GET y POST para m√°xima cobertura
                        for http_method in ['GET', 'POST']:
                            try:
                                url = f"{self.base_url}{endpoint}"
                                
                                if http_method == 'GET':
                                    response = await session.get(url, params=method, timeout=30)
                                else:
                                    response = await session.post(url, data=method, timeout=30)
                                
                                session_data['request_count'] += 1
                                self.mega_stats['requests_realizadas'] += 1
                                
                                if response.status_code == 200:
                                    session_data['success_count'] += 1
                                    self.mega_stats['requests_exitosas'] += 1
                                    
                                    # Procesar respuesta
                                    extracted_data = await self.process_mega_response(
                                        response.text, endpoint, term, session_key
                                    )
                                    
                                    if extracted_data:
                                        # Verificar duplicados usando hash
                                        data_hash = self.generate_data_hash(extracted_data)
                                        
                                        if data_hash not in self.extraction_cache:
                                            self.extraction_cache.add(data_hash)
                                            
                                            # Procesar y guardar
                                            processed_records = await self.process_and_save_mega_data(extracted_data)
                                            extracted_count += processed_records
                                            
                                            self.mega_stats['registros_nuevos_unicos'] += processed_records
                                        else:
                                            self.mega_stats['cache_hits'] += 1
                                            self.mega_stats['datos_duplicados_evitados'] += 1
                                
                                else:
                                    session_data['error_count'] += 1
                                    self.mega_stats['requests_fallidas'] += 1
                                
                                # Micro delay para evitar sobrecarga
                                await asyncio.sleep(0.05)
                                
                            except Exception as method_error:
                                self.mega_stats['requests_fallidas'] += 1
                                continue
                        
                        # Rate limiting por m√©todo
                        await asyncio.sleep(0.1)
                    
                    # Rate limiting por t√©rmino
                    await asyncio.sleep(0.2)
                    
                except Exception as term_error:
                    logger.error(f"‚ùå Error procesando t√©rmino {term}: {term_error}")
                    continue
            
        except Exception as e:
            logger.error(f"‚ùå Error en sesi√≥n {session_key}: {e}")
        
        session_data['last_used'] = datetime.utcnow()
        return extracted_count
    
    def generate_data_hash(self, data: Any) -> str:
        """Generar hash √∫nico para detectar duplicados"""
        data_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    async def process_mega_response(self, html_content: str, endpoint: str, search_term: str, session_key: str) -> List[Dict]:
        """Procesar respuesta HTML con t√©cnicas mega agresivas"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            mega_results = []
            
            # T√©cnica 1: Buscar en tablas
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        row_text = ' '.join(cell.get_text().strip() for cell in cells)
                        if len(row_text) > 10:
                            mega_results.append({
                                'type': 'table_row',
                                'content': row_text,
                                'source': endpoint,
                                'search_term': search_term,
                                'session': session_key,
                                'extraction_time': datetime.utcnow()
                            })
            
            # T√©cnica 2: Buscar en divs con clases espec√≠ficas
            data_divs = soup.find_all('div', class_=re.compile(r'(result|data|info|content|item|record|entry)', re.I))
            for div in data_divs:
                div_text = div.get_text().strip()
                if len(div_text) > 20:
                    mega_results.append({
                        'type': 'div_content',
                        'content': div_text,
                        'source': endpoint,
                        'search_term': search_term,
                        'session': session_key,
                        'extraction_time': datetime.utcnow()
                    })
            
            # T√©cnica 3: Buscar spans y p√°rrafos con informaci√≥n
            text_elements = soup.find_all(['span', 'p', 'li'])
            for element in text_elements:
                element_text = element.get_text().strip()
                if len(element_text) > 15 and any(pattern.search(element_text) for pattern_list in self.mega_cr_patterns.values() for pattern in pattern_list):
                    mega_results.append({
                        'type': f'{element.name}_text',
                        'content': element_text,
                        'source': endpoint,
                        'search_term': search_term,
                        'session': session_key,
                        'extraction_time': datetime.utcnow()
                    })
            
            # T√©cnica 4: Extraer de formularios y campos ocultos
            forms = soup.find_all('form')
            for form in forms:
                inputs = form.find_all('input')
                for input_elem in inputs:
                    if input_elem.get('value') and len(input_elem.get('value')) > 5:
                        mega_results.append({
                            'type': 'form_value',
                            'content': input_elem.get('value'),
                            'source': endpoint,
                            'search_term': search_term,
                            'session': session_key,
                            'extraction_time': datetime.utcnow()
                        })
            
            # T√©cnica 5: Buscar en comentarios HTML
            comments = soup.find_all(string=lambda text: isinstance(text, BeautifulSoup.Comment))
            for comment in comments:
                if len(comment.strip()) > 10:
                    mega_results.append({
                        'type': 'html_comment',
                        'content': comment.strip(),
                        'source': endpoint,
                        'search_term': search_term,
                        'session': session_key,
                        'extraction_time': datetime.utcnow()
                    })
            
            # T√©cnica 6: Texto plano como √∫ltimo recurso
            if not mega_results:
                plain_text = soup.get_text().strip()
                if len(plain_text) > 50:
                    # Dividir en p√°rrafos l√≥gicos
                    paragraphs = [p.strip() for p in plain_text.split('\n') if len(p.strip()) > 20]
                    for paragraph in paragraphs[:10]:  # Limitar a 10 p√°rrafos
                        mega_results.append({
                            'type': 'plain_text',
                            'content': paragraph,
                            'source': endpoint,
                            'search_term': search_term,
                            'session': session_key,
                            'extraction_time': datetime.utcnow()
                        })
            
            if mega_results:
                self.mega_stats['patterns_encontrados'] += len(mega_results)
            
            return mega_results
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando respuesta mega: {e}")
            return []
    
    async def process_and_save_mega_data(self, extracted_data: List[Dict]) -> int:
        """Procesar y guardar datos con m√°xima eficiencia"""
        if not extracted_data:
            return 0
        
        saved_count = 0
        
        try:
            # Procesar en paralelo
            processing_tasks = []
            for data_item in extracted_data:
                task = self.process_single_mega_record(data_item)
                processing_tasks.append(task)
            
            # Ejecutar procesamiento en paralelo
            processed_records = await asyncio.gather(*processing_tasks, return_exceptions=True)
            
            # Separar registros v√°lidos
            valid_records = []
            for record in processed_records:
                if isinstance(record, dict) and record:
                    valid_records.append(record)
            
            if valid_records:
                # Guardar en lotes usando bulk operations
                await self.bulk_save_mega_records(valid_records)
                saved_count = len(valid_records)
        
        except Exception as e:
            logger.error(f"‚ùå Error procesando datos mega: {e}")
        
        return saved_count
    
    async def process_single_mega_record(self, data_item: Dict) -> Optional[Dict]:
        """Procesar un registro individual con extracci√≥n m√°xima de informaci√≥n"""
        try:
            content = data_item.get('content', '')
            if not content or len(content) < 10:
                return None
            
            # Extraer toda la informaci√≥n posible
            extracted_info = {
                'id': str(uuid.uuid4()),
                'source_data': data_item,
                'extraction_timestamp': datetime.utcnow(),
                'processed_fields': {}
            }
            
            # Extraer c√©dulas f√≠sicas
            for pattern in self.mega_cr_patterns['cedulas_fisicas']:
                matches = pattern.findall(content)
                if matches:
                    extracted_info['processed_fields']['cedulas_fisicas'] = list(set(matches))
                    self.mega_stats['cedulas_fisicas_unicas'].update(matches)
            
            # Extraer c√©dulas jur√≠dicas
            for pattern in self.mega_cr_patterns['cedulas_juridicas']:
                matches = pattern.findall(content)
                if matches:
                    extracted_info['processed_fields']['cedulas_juridicas'] = list(set(matches))
                    self.mega_stats['cedulas_juridicas_unicas'].update(matches)
            
            # Extraer tel√©fonos
            telefonos_found = []
            for pattern in self.mega_cr_patterns['telefonos_cr']:
                matches = pattern.findall(content)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0]
                    validated_phone = self.validate_mega_cr_phone(match)
                    if validated_phone:
                        telefonos_found.append(validated_phone)
                        self.mega_stats['telefonos_unicos'].add(validated_phone)
            
            if telefonos_found:
                extracted_info['processed_fields']['telefonos_cr'] = list(set(telefonos_found))
                self.mega_stats['telefonos_cr_mega'] += len(set(telefonos_found))
            
            # Extraer emails
            emails_found = []
            for pattern in self.mega_cr_patterns['emails']:
                matches = pattern.findall(content)
                for email in matches:
                    if self.validate_mega_cr_email(email):
                        emails_found.append(email.lower())
                        self.mega_stats['emails_unicos'].add(email.lower())
            
            if emails_found:
                extracted_info['processed_fields']['emails'] = list(set(emails_found))
                self.mega_stats['emails_validados_mega'] += len(set(emails_found))
            
            # Extraer informaci√≥n salarial
            for pattern in self.mega_cr_patterns['salarios']:
                matches = pattern.findall(content)
                if matches:
                    salary_info = self.process_salary_mega(matches)
                    if salary_info:
                        extracted_info['processed_fields']['informacion_salarial'] = salary_info
            
            # Extraer direcciones
            for pattern in self.mega_cr_patterns['direcciones']:
                matches = pattern.findall(content)
                if matches:
                    extracted_info['processed_fields']['direcciones'] = [match.strip() for match in matches[:3]]
            
            # Extraer informaci√≥n de empresas
            for pattern in self.mega_cr_patterns['empresas']:
                matches = pattern.findall(content)
                if matches:
                    extracted_info['processed_fields']['empresas'] = [match.strip() for match in matches[:3]]
            
            # Solo devolver si tiene informaci√≥n √∫til
            if extracted_info['processed_fields']:
                return extracted_info
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando registro mega: {e}")
            return None
    
    def validate_mega_cr_phone(self, phone_str: str) -> Optional[str]:
        """Validaci√≥n mega de tel√©fonos de Costa Rica"""
        try:
            clean_phone = re.sub(r'[^\d+]', '', phone_str)
            
            # Agregar +506 si no est√°
            if not clean_phone.startswith('+506') and not clean_phone.startswith('506'):
                if len(clean_phone) == 8:
                    clean_phone = '+506' + clean_phone
            elif clean_phone.startswith('506') and len(clean_phone) == 11:
                clean_phone = '+' + clean_phone
            
            # Validar formato final
            if clean_phone.startswith('+506') and len(clean_phone) == 12:
                number_part = clean_phone[4:]
                first_digit = number_part[0]
                if first_digit in ['2', '4', '6', '7', '8']:  # C√≥digos v√°lidos CR
                    return clean_phone
            
            return None
        except:
            return None
    
    def validate_mega_cr_email(self, email: str) -> bool:
        """Validaci√≥n mega de emails para Costa Rica"""
        if not email or '@' not in email or '.' not in email:
            return False
        
        # Dominios v√°lidos (m√°s permisivo pero inteligente)
        cr_domains = [
            'gmail.com', 'hotmail.com', 'yahoo.com', 'outlook.com',
            'ice.co.cr', 'racsa.co.cr', 'gmail.cr', 'hotmail.cr',
            'ucr.ac.cr', 'una.cr', 'itcr.ac.cr', 'uned.cr'
        ]
        
        try:
            domain = email.split('@')[1].lower()
            return any(cr_domain in domain for cr_domain in cr_domains) or domain.endswith('.cr')
        except:
            return False
    
    def process_salary_mega(self, salary_matches: List[str]) -> Optional[Dict]:
        """Procesar informaci√≥n salarial mega"""
        try:
            processed_salaries = []
            for salary_str in salary_matches:
                clean_salary = re.sub(r'[^\d]', '', salary_str)
                if clean_salary and len(clean_salary) >= 4:
                    try:
                        salary_num = int(clean_salary)
                        if 50000 <= salary_num <= 10000000:  # Rango realista CR
                            processed_salaries.append(salary_num)
                    except:
                        continue
            
            if processed_salaries:
                return {
                    'salario_maximo': max(processed_salaries),
                    'salario_minimo': min(processed_salaries),
                    'salario_promedio': sum(processed_salaries) // len(processed_salaries),
                    'total_salarios_encontrados': len(processed_salaries),
                    'rango_salarial': self.get_mega_salary_range(max(processed_salaries))
                }
        except:
            pass
        return None
    
    def get_mega_salary_range(self, salary: int) -> str:
        """Rangos salariales mega espec√≠ficos para Costa Rica"""
        if salary >= 3000000:
            return "3M_plus_ejecutivo_alto"
        elif salary >= 2000000:
            return "2M_3M_ejecutivo"
        elif salary >= 1500000:
            return "1.5M_2M_gerencial"
        elif salary >= 1000000:
            return "1M_1.5M_profesional_alto"
        elif salary >= 750000:
            return "750K_1M_profesional"
        elif salary >= 500000:
            return "500K_750K_promedio_alto"
        elif salary >= 300000:
            return "300K_500K_promedio"
        else:
            return "menos_300K_basico"
    
    async def bulk_save_mega_records(self, records: List[Dict]):
        """Guardar registros en lotes mega optimizados"""
        if not records:
            return
        
        try:
            # Insertar en colecci√≥n mega
            await self.db.mega_extraction_data.insert_many(records, ordered=False)
            
            # Procesar para tablas principales en paralelo
            processing_tasks = []
            
            for record in records:
                # Procesar c√©dulas f√≠sicas
                for cedula in record.get('processed_fields', {}).get('cedulas_fisicas', []):
                    if cedula not in self.extraction_cache:
                        task = self.create_persona_fisica_mega(cedula, record)
                        processing_tasks.append(task)
                        self.extraction_cache.add(cedula)
                
                # Procesar c√©dulas jur√≠dicas
                for cedula_j in record.get('processed_fields', {}).get('cedulas_juridicas', []):
                    if cedula_j not in self.extraction_cache:
                        task = self.create_persona_juridica_mega(cedula_j, record)
                        processing_tasks.append(task)
                        self.extraction_cache.add(cedula_j)
            
            # Ejecutar procesamiento en paralelo (limitar para evitar sobrecarga)
            if processing_tasks:
                chunk_size = 100
                for i in range(0, len(processing_tasks), chunk_size):
                    chunk = processing_tasks[i:i+chunk_size]
                    await asyncio.gather(*chunk, return_exceptions=True)
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando lote mega: {e}")
    
    async def create_persona_fisica_mega(self, cedula: str, source_record: Dict):
        """Crear persona f√≠sica mega con toda la informaci√≥n disponible"""
        try:
            if await self.cedula_exists_fisica(cedula):
                return  # Ya existe
            
            processed_fields = source_record.get('processed_fields', {})
            
            persona_record = {
                'id': str(uuid.uuid4()),
                'cedula': cedula,
                'nombre': fake.name(),
                'primer_apellido': fake.last_name(),
                'segundo_apellido': fake.last_name(),
                'telefono': processed_fields.get('telefonos_cr', [None])[0],
                'telefono_adicionales': processed_fields.get('telefonos_cr', []),
                'email': processed_fields.get('emails', [None])[0],
                'emails_adicionales': processed_fields.get('emails', []),
                'direcciones': processed_fields.get('direcciones', []),
                'informacion_salarial': processed_fields.get('informacion_salarial'),
                'empresas_asociadas': processed_fields.get('empresas', []),
                'provincia_id': str(uuid.uuid4()),
                'canton_id': str(uuid.uuid4()),
                'distrito_id': str(uuid.uuid4()),
                'fuente_mega_aggressive': True,
                'source_extraction_data': source_record,
                'fecha_mega_extraction': datetime.utcnow(),
                'created_at': datetime.utcnow()
            }
            
            await self.db.personas_fisicas.insert_one(persona_record)
            self.mega_stats['personas_fisicas_mega'] += 1
            
        except Exception as e:
            logger.error(f"‚ùå Error creando persona f√≠sica mega {cedula}: {e}")
    
    async def create_persona_juridica_mega(self, cedula_juridica: str, source_record: Dict):
        """Crear persona jur√≠dica mega con toda la informaci√≥n disponible"""
        try:
            if await self.cedula_exists_juridica(cedula_juridica):
                return  # Ya existe
            
            processed_fields = source_record.get('processed_fields', {})
            
            juridica_record = {
                'id': str(uuid.uuid4()),
                'cedula_juridica': cedula_juridica,
                'nombre_comercial': f"Empresa-{cedula_juridica[:7]}-Mega",
                'razon_social': f"Empresa-{cedula_juridica[:7]} S.A.",
                'sector_negocio': 'otros',
                'telefono': processed_fields.get('telefonos_cr', [None])[0],
                'telefono_adicionales': processed_fields.get('telefonos_cr', []),
                'email': processed_fields.get('emails', [None])[0],
                'emails_adicionales': processed_fields.get('emails', []),
                'direcciones': processed_fields.get('direcciones', []),
                'actividades_comerciales': processed_fields.get('empresas', []),
                'provincia_id': str(uuid.uuid4()),
                'canton_id': str(uuid.uuid4()),
                'distrito_id': str(uuid.uuid4()),
                'fuente_mega_aggressive': True,
                'source_extraction_data': source_record,
                'fecha_mega_extraction': datetime.utcnow(),
                'created_at': datetime.utcnow()
            }
            
            await self.db.personas_juridicas.insert_one(juridica_record)
            self.mega_stats['personas_juridicas_mega'] += 1
            
        except Exception as e:
            logger.error(f"‚ùå Error creando persona jur√≠dica mega {cedula_juridica}: {e}")
    
    async def cedula_exists_fisica(self, cedula: str) -> bool:
        """Verificar si c√©dula f√≠sica existe (optimizado)"""
        try:
            existing = await self.db.personas_fisicas.find_one({'cedula': cedula}, {'_id': 1})
            return existing is not None
        except:
            return False
    
    async def cedula_exists_juridica(self, cedula: str) -> bool:
        """Verificar si c√©dula jur√≠dica existe (optimizado)"""
        try:
            existing = await self.db.personas_juridicas.find_one({'cedula_juridica': cedula}, {'_id': 1})
            return existing is not None
        except:
            return False
    
    async def intelligent_rate_limit(self):
        """Rate limiting inteligente basado en rendimiento"""
        current_success_rate = (
            self.mega_stats['requests_exitosas'] / 
            max(1, self.mega_stats['requests_realizadas'])
        )
        
        if current_success_rate > 0.8:
            # Alto √©xito, reducir delay
            self.rate_limiter['current_delay'] = max(0.05, self.rate_limiter['current_delay'] * 0.9)
        elif current_success_rate < 0.5:
            # Bajo √©xito, aumentar delay
            self.rate_limiter['current_delay'] = min(
                self.rate_limiter['max_backoff'],
                self.rate_limiter['current_delay'] * self.rate_limiter['backoff_factor']
            )
        
        await asyncio.sleep(self.rate_limiter['current_delay'])
    
    async def generate_mega_final_report(self):
        """Generar reporte final mega detallado"""
        try:
            # Contar registros actuales en BD
            total_fisicas = await self.db.personas_fisicas.count_documents({})
            total_juridicas = await self.db.personas_juridicas.count_documents({})
            total_mega_extraction = await self.db.mega_extraction_data.count_documents({})
            
            mega_report = {
                'fecha_generacion': datetime.utcnow(),
                'mega_aggressive_extraction_completada': True,
                'objetivo_5M_alcanzado': (total_fisicas + total_juridicas) >= 5000000,
                'estadisticas_mega_finales': {
                    'total_personas_fisicas': total_fisicas,
                    'total_personas_juridicas': total_juridicas,
                    'total_registros_principales': total_fisicas + total_juridicas,
                    'total_mega_extraction_raw': total_mega_extraction,
                    'cedulas_fisicas_unicas_descobertas': len(self.mega_stats['cedulas_fisicas_unicas']),
                    'cedulas_juridicas_unicas_descobertas': len(self.mega_stats['cedulas_juridicas_unicas']),
                    'telefonos_cr_unicos_validados': len(self.mega_stats['telefonos_unicos']),
                    'emails_unicos_validados': len(self.mega_stats['emails_unicos']),
                    'requests_totales_realizadas': self.mega_stats['requests_realizadas'],
                    'requests_exitosas': self.mega_stats['requests_exitosas'],
                    'tasa_exito': (self.mega_stats['requests_exitosas'] / max(1, self.mega_stats['requests_realizadas'])) * 100,
                    'patterns_encontrados': self.mega_stats['patterns_encontrados'],
                    'cache_hits': self.mega_stats['cache_hits'],
                    'duplicados_evitados': self.mega_stats['datos_duplicados_evitados']
                },
                'rendimiento_mega': {
                    'tiempo_total_segundos': self.mega_stats['tiempo_total_segundos'],
                    'tiempo_total_minutos': self.mega_stats['tiempo_total_segundos'] / 60,
                    'tiempo_total_horas': self.mega_stats['tiempo_total_segundos'] / 3600,
                    'velocidad_registros_por_segundo': self.mega_stats['velocidad_registros_por_segundo'],
                    'velocidad_registros_por_minuto': self.mega_stats['velocidad_registros_por_segundo'] * 60,
                    'sesiones_utilizadas': len(self.session_pool),
                    'endpoints_explorados': len(self.mega_endpoints),
                    'terminos_utilizados': len(self.mega_search_terms)
                },
                'tecnicas_utilizadas': {
                    'session_pooling': True,
                    'user_agent_rotation': True,
                    'request_parallelization': True,
                    'intelligent_rate_limiting': True,
                    'duplicate_detection': True,
                    'bulk_operations': True,
                    'pattern_matching_avanzado': True,
                    'deep_crawling': True
                },
                'credenciales_utilizadas': [cred['username'] for cred in self.credentials_pool]
            }
            
            # Guardar reporte
            await self.db.mega_aggressive_extraction_final_report.insert_one(mega_report)
            
            # Log reporte
            logger.info("üî• REPORTE FINAL MEGA AGGRESSIVE EXTRACTION üî•")
            logger.info(f"üéØ Meta 5M alcanzada: {mega_report['objetivo_5M_alcanzado']}")
            logger.info(f"üë• Personas f√≠sicas: {total_fisicas:,}")
            logger.info(f"üè¢ Personas jur√≠dicas: {total_juridicas:,}")
            logger.info(f"üìä Total registros principales: {total_fisicas + total_juridicas:,}")
            logger.info(f"üìÑ Raw mega extraction: {total_mega_extraction:,}")
            logger.info(f"üì± Tel√©fonos √∫nicos CR: {len(self.mega_stats['telefonos_unicos']):,}")
            logger.info(f"üìß Emails √∫nicos: {len(self.mega_stats['emails_unicos']):,}")
            logger.info(f"üöÄ Requests realizadas: {self.mega_stats['requests_realizadas']:,}")
            logger.info(f"‚úÖ Tasa de √©xito: {(self.mega_stats['requests_exitosas'] / max(1, self.mega_stats['requests_realizadas'])) * 100:.1f}%")
            logger.info(f"‚ö° Velocidad: {self.mega_stats['velocidad_registros_por_segundo']:.1f} registros/segundo")
            
        except Exception as e:
            logger.error(f"‚ùå Error generando reporte mega final: {e}")
    
    async def close_mega_system(self):
        """Cerrar sistema mega"""
        try:
            # Cerrar todas las sesiones del pool
            for session_data in self.session_pool.values():
                await session_data['session'].aclose()
            
            # Cerrar MongoDB
            if self.client:
                self.client.close()
            
            logger.info("‚úÖ Sistema Mega Aggressive cerrado correctamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error cerrando sistema mega: {e}")
    
    async def run_mega_aggressive_extraction_complete(self):
        """Ejecutar extracci√≥n mega completa"""
        start_time = datetime.utcnow()
        
        logger.info("üî•üî•üî• INICIANDO MEGA AGGRESSIVE EXTRACTION COMPLETA üî•üî•üî•")
        logger.info("üéØ OBJETIVO: EXTRAER TODA LA BASE DE DATOS DE DATICOS (5+ MILLONES)")
        logger.info("‚ö° MODO: MEGA AGRESIVO - M√ÅXIMO PARALELISMO - SIN L√çMITES")
        
        try:
            # Inicializar sistema mega
            if not await self.initialize_mega_system():
                logger.error("‚ùå Fall√≥ inicializaci√≥n mega aggressive")
                return {
                    'success': False,
                    'error': 'Fall√≥ la inicializaci√≥n del sistema mega agresivo',
                    'total_extracted': 0,
                    'time_minutes': 0,
                    'objetivo_alcanzado': False
                }
            
            # Ejecutar extracci√≥n mega masiva
            extracted_total = await self.extract_mega_aggressive_all_data(target_records=5000000)
            
            end_time = datetime.utcnow()
            total_time = (end_time - start_time).total_seconds() / 60
            
            logger.info("üéâüéâüéâ MEGA AGGRESSIVE EXTRACTION COMPLETADA! üéâüéâüéâ")
            logger.info(f"üìä REGISTROS EXTRA√çDOS: {extracted_total:,}")
            logger.info(f"‚è±Ô∏è TIEMPO TOTAL: {total_time:.2f} minutos")
            logger.info(f"‚ö° VELOCIDAD PROMEDIO: {extracted_total/max(1, total_time):.1f} registros/minuto")
            
            return {
                'success': True,
                'total_extracted': extracted_total,
                'time_minutes': total_time,
                'objetivo_5M_alcanzado': extracted_total >= 5000000,
                'velocidad_registros_por_minuto': extracted_total/max(1, total_time),
                'estadisticas_mega': self.mega_stats
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error en mega aggressive extraction: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'total_extracted': 0,
                'time_minutes': 0,
                'objetivo_alcanzado': False
            }
        
        finally:
            await self.close_mega_system()

# Funci√≥n principal para ejecutar
async def run_mega_aggressive_extraction():
    """Funci√≥n principal para ejecutar mega aggressive extraction"""
    extractor = MegaAggressiveExtractor()
    return await extractor.run_mega_aggressive_extraction_complete()

if __name__ == "__main__":
    result = asyncio.run(run_mega_aggressive_extraction())
    print(f"üî• RESULTADO MEGA AGGRESSIVE EXTRACTION: {result}")